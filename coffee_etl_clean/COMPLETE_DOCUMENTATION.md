# COFFEE ETL PIPELINE - T√ÄI LI·ªÜU HO√ÄN CH·ªàNH

**Phi√™n b·∫£n**: 2.0  
**Ng√†y**: 2025-12-07

---

## üìö C·∫§U TR√öC T√ÄI LI·ªÜU

T√†i li·ªáu ƒë∆∞·ª£c chia th√†nh 3 ph·∫ßn:

1. **ARCHITECTURE_PART1.md**: T·ªïng quan, ki·∫øn tr√∫c, design patterns
2. **ARCHITECTURE_PART2.md**: C·∫•u tr√∫c d·ª± √°n, chi ti·∫øt modules
3. **COMPLETE_DOCUMENTATION.md**: File n√†y - T·ªïng h·ª£p v√† b·ªï sung

---

## üéØ LU·ªíNG X·ª¨ L√ù CHI TI·∫æT

### B∆∞·ªõc 1: Kh·ªüi t·∫°o Pipeline

```python
class MainETLPipeline:
    def __init__(self):
        # 1. T·∫°o run_id theo th·ªùi gian
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 2. T·∫°o database name ƒë·ªông
        self.db_name = f"DB_{self.run_id}"
        
        # 3. Kh·ªüi t·∫°o loggers
        self.failed_logger = FailedDataLogger(self.run_id)
        self.entity_logger = EntityLogger(self.run_id)
        
        # 4. Kh·ªüi t·∫°o stats tracking
        self.stats = {
            "produced": {},
            "consumed": {},
            "valid": {},
            "invalid": {},
            "loaded": {}
        }
```

### B∆∞·ªõc 2: Setup Database

```python
def setup_database(self):
    # 1. K·∫øt n·ªëi master database
    master_db = SQLServerClient(database="master")
    master_db.connect()
    
    # 2. T·∫°o database m·ªõi
    DatabaseManager.create_database(self.db_name, master_db)
    
    # 3. K·∫øt n·ªëi database m·ªõi
    new_db = SQLServerClient(database=self.db_name)
    new_db.connect()
    
    # 4. T·∫°o schema staging
    DatabaseManager.create_staging_schema(new_db)
    
    # 5. T·∫°o staging tables (CSV + SQL)
    DatabaseManager.create_staging_tables(new_db)
```

### B∆∞·ªõc 3: Producer Phase

```python
def producer_phase(self):
    with RabbitMQClient() as rabbitmq:
        # 1. Producer t·ª´ CSV
        csv_stats = self.produce_from_csv(rabbitmq)
        
        # 2. Producer t·ª´ SQL
        sql_stats = self.produce_from_sql(rabbitmq)
        
        # 3. Merge stats
        self.stats["produced"].update(csv_stats)
        self.stats["produced"].update(sql_stats)
```

**Chi ti·∫øt produce_from_csv**:
```python
def produce_from_csv(self, rabbitmq):
    csv_files = {
        "khachhang.csv": "queue_khach_hang",
        "loaisanpham.csv": "queue_loai_mon",
        "tensanpham.csv": "queue_mon",
        "nguyenlieu.csv": "queue_nguyen_lieu",
        "dathang.csv": "queue_dat_hang"
    }
    
    for file_name, queue_name in csv_files.items():
        # 1. Declare queue
        rabbitmq.declare_queue(queue_name, durable=True)
        
        # 2. ƒê·ªçc CSV
        for row in csv_staging_reader(file_path):
            # 3. T·∫°o message
            message = {
                "source": "csv",
                "entity_type": queue_name.replace("queue_", ""),
                "data": row,
                "metadata": {
                    "file": file_name,
                    "extract_time": datetime.now().isoformat()
                }
            }
            
            # 4. Publish
            rabbitmq.publish(queue_name, message, persistent=True)
```

### B∆∞·ªõc 4: Consumer Phase

```python
def consumer_phase(self):
    # 1. K·∫øt n·ªëi Target DB
    self.target_db = SQLServerClient(database=self.db_name)
    self.target_db.connect()
    
    # 2. X·ª≠ l√Ω t·ª´ng queue
    queues = [
        ("queue_khach_hang", "khach_hang"),
        ("queue_loai_mon", "loai_mon"),
        ("queue_mon", "mon"),
        ("queue_nguyen_lieu", "nguyen_lieu"),
        ("queue_dat_hang", "dat_hang")
    ]
    
    for queue_name, entity_type in queues:
        self.consume_and_process(queue_name, entity_type)
```

**Chi ti·∫øt consume_and_process**:
```python
def consume_and_process(self, queue_name, entity_type):
    with RabbitMQClient() as rabbitmq:
        # 1. Ki·ªÉm tra s·ªë message
        message_count = rabbitmq.channel.queue_declare(
            queue=queue_name, passive=True
        ).method.message_count
        
        # 2. Kh·ªüi t·∫°o tracking
        csv_valid_rows = []
        sql_valid_rows = []
        invalid_rows = []
        seen_ids = set()
        seen_emails = set()
        
        # 3. Define callback
        def callback(ch, method, properties, body):
            message = json.loads(body.decode("utf-8"))
            data = message.get("data", {})
            source = message.get("source", "unknown")
            
            # 4. Validate
            is_valid, fixed_row, errors = rule_registry.validate_row(
                entity_type=entity_type,
                row=data,
                context={
                    "existing_ids": seen_ids,
                    "existing_emails": seen_emails,
                    "source": source  # ‚Üê Quan tr·ªçng!
                }
            )
            
            # 5. Ph√¢n lo·∫°i
            if is_valid:
                if source == "csv":
                    csv_valid_rows.append(fixed_row)
                else:
                    sql_valid_rows.append(fixed_row)
                
                # Track IDs
                if "id" in fixed_row:
                    seen_ids.add(int(fixed_row["id"]))
                if "email" in fixed_row:
                    seen_emails.add(fixed_row["email"].lower())
            else:
                invalid_rows.append((data, errors))
                self.failed_logger.add_failed_record(entity_type, data, errors)
                self.entity_logger.log_invalid_row(entity_type, consumed+1, errors, data)
            
            # 6. ACK
            ch.basic_ack(delivery_tag=method.delivery_tag)
        
        # 7. Consume
        rabbitmq.channel.basic_consume(queue=queue_name, on_message_callback=callback)
        while consumed < message_count:
            rabbitmq.connection.process_data_events(time_limit=1)
        
        # 8. Transform & Load
        if csv_valid_rows:
            self.transform_and_load(entity_type, csv_valid_rows, source="csv")
        if sql_valid_rows:
            self.transform_and_load(entity_type, sql_valid_rows, source="sql")
```

### B∆∞·ªõc 5: Transform & Load

```python
def transform_and_load(self, entity_type, rows, source="csv"):
    # 1. Transform
    transformed_rows = []
    for row in rows:
        transformed_rows.append(
            DataTransformer.transform(entity_type, row)
        )
    
    # 2. Determine staging table
    suffix = "_csv" if source == "csv" else "_sql"
    staging_table = f"staging.{entity_type}{suffix}"
    
    # 3. Bulk insert
    loaded = self.target_db.bulk_insert(
        table_name=staging_table,
        data=transformed_rows,
        batch_size=1000
    )
    
    # 4. Update stats
    self.stats["loaded"][f"{entity_type}_{source}"] = loaded
```

---

## üîç DATA QUALITY SYSTEM

### Validation Flow

```
Row Data
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RuleRegistry.validate_row()         ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ 1. Detect source (CSV vs SQL)      ‚îÇ
‚îÇ 2. Route to appropriate validator  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Entity-specific Validator           ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ ‚Ä¢ khach_hang_rules.py               ‚îÇ
‚îÇ ‚Ä¢ mon_rules.py / mon_csv_rules.py   ‚îÇ
‚îÇ ‚Ä¢ nguyen_lieu_rules.py              ‚îÇ
‚îÇ ‚Ä¢ loai_mon_rules.py                 ‚îÇ
‚îÇ ‚Ä¢ dat_hang_rules.py                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Validation Rules (80+)              ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ ‚Ä¢ Regex-based (30+ patterns)        ‚îÇ
‚îÇ ‚Ä¢ Business rules                    ‚îÇ
‚îÇ ‚Ä¢ Foreign key checks                ‚îÇ
‚îÇ ‚Ä¢ Duplicate detection               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
(is_valid, fixed_row, errors)
```

### Validation Rules Summary

**Kh√°ch h√†ng (khach_hang)**: 30 rules
- ID: 8 rules
- H·ªç t√™n: 10 rules
- SƒêT: 7 rules
- Email: 6 rules
- Th√†nh ph·ªë: 5 rules

**M√≥n ƒÉn (mon)**: 15 rules
- ID: 2 rules
- T√™n m√≥n: 5 rules
- Lo·∫°i ID: 4 rules
- Gi√°: 4 rules

**M√≥n ƒÉn CSV (mon_csv)**: 12 rules
- ID: 2 rules (c√≥ th·ªÉ r·ªóng)
- T√™n s·∫£n ph·∫©m: 5 rules
- Gi√°: 4 rules
- Lo·∫°i (t√™n): 3 rules

**Nguy√™n li·ªáu (nguyen_lieu)**: 17 rules
- ID: 2 rules
- T√™n nguy√™n li·ªáu: 5 rules
- S·ªë l∆∞·ª£ng: 4 rules
- ƒê∆°n v·ªã: 3 rules
- Nh√† cung c·∫•p: 3 rules

**Lo·∫°i m√≥n (loai_mon)**: 12 rules
- ID: 4 rules
- T√™n lo·∫°i: 5 rules
- M√¥ t·∫£: 3 rules

**ƒê·∫∑t h√†ng (dat_hang)**: 20 rules
- ID: 2 rules
- Kh√°ch h√†ng ID: 3 rules
- M√≥n ID: 3 rules
- S·ªë l∆∞·ª£ng: 4 rules
- Ng√†y ƒë·∫∑t: 4 rules
- Tr·∫°ng th√°i: 3 rules

**T·ªîNG: 106 rules**

---

## üîÑ TRANSFORMATION LOGIC

### Transform Flow

```
Raw Data (CSV/SQL)
    ‚Üì
DataTransformer.transform(entity_type, row)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Detect Format                       ‚îÇ
‚îÇ ‚Ä¢ CSV: c√≥ ten_san_pham              ‚îÇ
‚îÇ ‚Ä¢ SQL: c√≥ ten_mon                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Transform Operations                ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ ‚Ä¢ Type conversion                   ‚îÇ
‚îÇ ‚Ä¢ Field mapping                     ‚îÇ
‚îÇ ‚Ä¢ Data cleaning                     ‚îÇ
‚îÇ ‚Ä¢ Normalization                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Transformed Data (ready for staging)
```

### Transform Examples

**Kh√°ch h√†ng**:
```python
# Input (CSV)
{
    "customer_id": "  123  ",
    "ho_ten": "NGUYEN VAN A",
    "sdt": "0123456789",
    "email": "Test@Gmail.COM",
    "thanh_pho": "ha noi"
}

# Output (Transformed)
{
    "customer_id": "123",
    "ho_ten": "Nguyen Van A",  # Title case
    "sdt": "0123456789",
    "email": "test@gmail.com",  # Lowercase
    "thanh_pho": "H√† N·ªôi",
    "extract_time": datetime(2025, 12, 7, 14, 30, 22)
}
```

**M√≥n ƒÉn (CSV)**:
```python
# Input (CSV)
{
    "id": "1",
    "ten_san_pham": "B√°nh m√¨",
    "gia": "25000",
    "loai": "ƒÇn s√°ng"
}

# Output (Transformed)
{
    "ten_mon": "B√°nh m√¨",  # Mapped from ten_san_pham
    "loai_name": "ƒÇn s√°ng",  # T√™n lo·∫°i
    "loai_id": None,  # C·∫ßn lookup sau
    "gia": 25000.0,  # Float
    "extract_time": datetime(...)
}
```

---

## üìä STAGING TABLES

### Schema Design

**Ph√¢n chia theo ngu·ªìn**:
- `staging.*_csv`: D·ªØ li·ªáu t·ª´ CSV
- `staging.*_sql`: D·ªØ li·ªáu t·ª´ SQL Server

**L·ª£i √≠ch**:
1. Traceability: Bi·∫øt d·ªØ li·ªáu t·ª´ ngu·ªìn n√†o
2. Data lineage: Track ngu·ªìn g·ªëc
3. Reconciliation: So s√°nh 2 ngu·ªìn
4. Debugging: D·ªÖ debug khi c√≥ v·∫•n ƒë·ªÅ

### Table Structure

```sql
-- staging.khach_hang_csv
CREATE TABLE staging.khach_hang_csv (
    id INT IDENTITY(1,1) PRIMARY KEY,
    customer_id NVARCHAR(50),
    ho_ten NVARCHAR(200),
    sdt NVARCHAR(20),
    thanh_pho NVARCHAR(100),
    email NVARCHAR(200),
    source_system NVARCHAR(50),
    [file] NVARCHAR(200),
    line NVARCHAR(50),
    extract_time DATETIME,
    loaded_at DATETIME DEFAULT GETDATE(),
    updated_at DATETIME DEFAULT GETDATE()
);

-- staging.khach_hang_sql (c√πng c·∫•u tr√∫c)
```

---

## üö® ERROR HANDLING

### Retry Mechanism

```python
@retry(times=3, delay_sec=2, label="operation")
def risky_operation():
    # Attempt 1: Try
    # Attempt 2: Wait 2s, try again
    # Attempt 3: Wait 2s, try again
    # If all fail: Raise exception
```

**√Åp d·ª•ng cho**:
- Database connections
- RabbitMQ connections
- Network operations

### Transaction Management

```python
try:
    # Begin transaction (implicit)
    self.cursor.executemany(query, values)
    self.connection.commit()  # Success
except Exception as e:
    self.connection.rollback()  # Rollback on error
    raise
```

### Failed Data Logging

**2 c·∫•p ƒë·ªô logging**:

1. **FailedDataLogger**: T·ªïng h·ª£p t·∫•t c·∫£
```csv
Time,Entity,Errors,Data
14:30:22,khach_hang,"email: Sai ƒë·ªãnh d·∫°ng",customer_id=123|ho_ten=Test
```

2. **EntityLogger**: Chi ti·∫øt t·ª´ng entity
```json
{
    "timestamp": "2025-12-07 14:30:22",
    "row_number": 5,
    "errors": ["email: Sai ƒë·ªãnh d·∫°ng"],
    "data": {"customer_id": "123", "email": "invalid"}
}
```

---

## üìù LOGGING SYSTEM

### Log Levels

```python
logger.info("Normal operation")      # INFO
logger.warning("Potential issue")    # WARNING
logger.error("Error occurred")       # ERROR
```

### Log Files

```
logs/
‚îú‚îÄ‚îÄ run_20251207_143022/
‚îÇ   ‚îú‚îÄ‚îÄ failed_data.csv              # Failed records
‚îÇ   ‚îú‚îÄ‚îÄ khach_hang_validation.log    # Entity-specific
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ pipeline.log                     # General pipeline
‚îú‚îÄ‚îÄ data.log                         # Data processing
‚îî‚îÄ‚îÄ error.log                        # Errors only
```

### Log Format (JSON)

```json
{
    "time": "2025-12-07 14:30:22",
    "level": "INFO",
    "message": "Producer phase started"
}
```

---

## üéì BEST PRACTICES

### 1. Code Organization
- ‚úÖ Modular design
- ‚úÖ Single Responsibility Principle
- ‚úÖ DRY (Don't Repeat Yourself)

### 2. Error Handling
- ‚úÖ Try-catch ·ªü m·ªçi operations
- ‚úÖ Retry cho transient errors
- ‚úÖ Transaction management

### 3. Logging
- ‚úÖ Structured logging (JSON)
- ‚úÖ Multiple log levels
- ‚úÖ Rotating logs

### 4. Testing
- ‚úÖ Unit tests cho validators
- ‚úÖ Integration tests cho pipeline
- ‚úÖ Test data quality rules

### 5. Performance
- ‚úÖ Bulk insert (batch 1000)
- ‚úÖ Generator pattern cho CSV
- ‚úÖ Connection pooling

---

## üìö T√ÄI LI·ªÜU THAM KH·∫¢O

1. **ARCHITECTURE_PART1.md**: Ki·∫øn tr√∫c, design patterns
2. **ARCHITECTURE_PART2.md**: C·∫•u tr√∫c, modules
3. **README.md**: H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
4. **Code comments**: Inline documentation

---

**H·∫æT T√ÄI LI·ªÜU**
