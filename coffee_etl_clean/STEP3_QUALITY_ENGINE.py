"""
STEP 3: QUALITY ENGINE
======================
ƒê·ªçc staging/raw/*.csv ‚Üí Validate ‚Üí Ph√¢n lo·∫°i v√†o CLEAN/ERROR zones

Lu·ªìng:
- staging/raw/*.csv ‚Üí Quality Engine (106 rules) ‚Üí staging/clean/*.csv + staging/error/*.csv

Output:
- staging/clean/*.csv (Valid records)
- staging/error/*.csv (Invalid records v·ªõi error messages)
"""

import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set

from etl.quality.rule_registry import rule_registry
from etl.logger import logger


class QualityEnginePipeline:
    """Pipeline Quality Engine - Validate v√† ph√¢n lo·∫°i d·ªØ li·ªáu."""
    
    def __init__(self):
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Th∆∞ m·ª•c
        self.raw_dir = Path("staging") / "raw"
        self.clean_dir = Path("staging") / "clean"
        self.error_dir = Path("staging") / "error"
        
        self.clean_dir.mkdir(parents=True, exist_ok=True)
        self.error_dir.mkdir(parents=True, exist_ok=True)
        
        self.stats = {}
        
        # Store validated data in memory ƒë·ªÉ truy·ªÅn cho STEP 4
        self.validated_data = {}  # {entity_source: [valid_rows]}
    
    def run(self):
        logger.info("=" * 80)
        logger.info("STEP 3: QUALITY ENGINE PIPELINE")
        logger.info("Run ID: %s", self.run_id)
        logger.info("Input: staging/raw/")
        logger.info("Output: staging/clean/ + staging/error/ + Memory")
        logger.info("=" * 80)
        
        try:
            # T√¨m t·∫•t c·∫£ CSV files trong raw/
            raw_files = list(self.raw_dir.glob("*.csv"))
            
            if not raw_files:
                logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file n√†o trong staging/raw/")
                return {}
            
            logger.info("\nüìÑ Found %s raw files", len(raw_files))
            
            for raw_file in sorted(raw_files):
                logger.info("\nüì• Processing: %s", raw_file.name)
                self.process_file(raw_file)
            
            self.print_summary()
            
            # Tr·∫£ v·ªÅ validated data ƒë·ªÉ STEP 4 s·ª≠ d·ª•ng
            logger.info("\n‚úÖ Validated data ready in memory for STEP 4")
            return self.validated_data
            
        except Exception as e:
            logger.error("‚ùå L·ªói Quality Engine pipeline: %s", e, exc_info=True)
            raise
    
    def process_file(self, raw_file: Path):
        """X·ª≠ l√Ω m·ªôt raw file."""
        # Parse file name: entity_source_runid.csv
        file_name = raw_file.stem  # B·ªè .csv
        parts = file_name.split("_")
        
        # X√°c ƒë·ªãnh entity type v√† source
        if len(parts) >= 2:
            # Tr∆∞·ªùng h·ª£p: khach_hang_csv_20251209_123456
            if parts[-2].isdigit():  # C√≥ run_id
                source = parts[-3]  # csv ho·∫∑c sql
                entity_type = "_".join(parts[:-3])  # khach_hang
            else:
                source = parts[-1]  # csv ho·∫∑c sql
                entity_type = "_".join(parts[:-1])  # khach_hang
        else:
            logger.warning("   ‚ö†Ô∏è  Kh√¥ng parse ƒë∆∞·ª£c file name: %s", file_name)
            return
        
        logger.info("   Entity: %s | Source: %s", entity_type, source)
        
        # ƒê·ªçc raw file
        try:
            with open(raw_file, "r", encoding="utf-8-sig") as f:
                reader = csv.DictReader(f)
                rows = list(reader)
        except Exception as e:
            logger.error("   ‚úó L·ªói ƒë·ªçc file: %s", e)
            return
        
        logger.info("   Total rows: %s", len(rows))
        
        # Validate t·ª´ng row
        valid_rows = []
        error_rows = []
        
        # Context cho validation (track IDs, emails ƒë·ªÉ check duplicate)
        seen_ids = set()
        seen_emails = set()
        
        for i, row in enumerate(rows, 1):
            # Lo·∫°i b·ªè metadata columns (_source, _extract_time, _run_id)
            data = {k: v for k, v in row.items() if not k.startswith("_")}
            
            # Validate
            is_valid, fixed_row, errors = rule_registry.validate_row(
                entity_type=entity_type,
                row=data,
                context={
                    "existing_ids": seen_ids,
                    "existing_emails": seen_emails,
                    "source": source
                }
            )
            
            if is_valid:
                # ‚úÖ VALID: Th√™m v√†o clean zone
                valid_rows.append(fixed_row)
                
                # Track IDs v√† emails ƒë·ªÉ check duplicate
                for id_field in ["id", "customer_id", "ma_nguyen_lieu", "ma_loai"]:
                    if id_field in fixed_row and fixed_row[id_field]:
                        try:
                            seen_ids.add(int(fixed_row[id_field]))
                        except (ValueError, TypeError):
                            pass
                
                if "email" in fixed_row and fixed_row["email"]:
                    seen_emails.add(fixed_row["email"].lower())
            else:
                # ‚ùå INVALID: Th√™m v√†o error zone
                # D√≤ng n√†y s·∫Ω KH√îNG ƒë∆∞·ª£c transform v√† KH√îNG ƒë∆∞·ª£c load v√†o SQL
                error_row = {**data}
                error_row["_errors"] = " | ".join(errors)
                error_row["_row_number"] = i
                error_rows.append(error_row)
        
        # Ghi v√†o clean/error files
        clean_file = self.clean_dir / f"{entity_type}_{source}_{self.run_id}.csv"
        error_file = self.error_dir / f"{entity_type}_{source}_{self.run_id}.csv"
        
        if valid_rows:
            # ‚úÖ Ch·ªâ c√°c rows VALID ƒë∆∞·ª£c ghi v√†o CLEAN zone
            # C√°c rows n√†y s·∫Ω ƒë∆∞·ª£c transform v√† load v√†o SQL ·ªü STEP 4
            self.write_csv(clean_file, valid_rows)
            logger.info("   ‚úì Clean: %s rows ‚Üí %s", len(valid_rows), clean_file.name)
        
        if error_rows:
            # ‚ùå C√°c rows INVALID ƒë∆∞·ª£c ghi v√†o ERROR zone
            # C√°c rows n√†y s·∫Ω KH√îNG ƒë∆∞·ª£c transform v√† KH√îNG ƒë∆∞·ª£c load v√†o SQL
            self.write_csv(error_file, error_rows)
            logger.info("   ‚úó Error: %s rows ‚Üí %s", len(error_rows), error_file.name)
        
        # Stats
        self.stats[file_name] = {
            "total": len(rows),
            "valid": len(valid_rows),
            "invalid": len(error_rows),
            "entity": entity_type,
            "source": source
        }
        
        # Store validated data in memory
        key = f"{entity_type}_{source}"
        self.validated_data[key] = valid_rows
    
    def write_csv(self, file_path: Path, rows: List[Dict]):
        """Ghi rows v√†o CSV file."""
        if not rows:
            return
        
        with open(file_path, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)
    
    def print_summary(self):
        logger.info("\n" + "=" * 80)
        logger.info("üìä QUALITY ENGINE SUMMARY")
        logger.info("=" * 80)
        
        logger.info("\nüìÅ Output Directories:")
        logger.info("   ‚Ä¢ Clean: %s", self.clean_dir)
        logger.info("   ‚Ä¢ Error: %s", self.error_dir)
        
        logger.info("\nüìä Validation Results:")
        total_all = 0
        total_valid = 0
        total_invalid = 0
        
        for file_name, stats in sorted(self.stats.items()):
            logger.info("   ‚Ä¢ %s (%s):", stats["entity"], stats["source"])
            logger.info("     Total: %s | Valid: %s | Invalid: %s", 
                       stats["total"], 
                       stats["valid"], 
                       stats["invalid"])
            
            total_all += stats["total"]
            total_valid += stats["valid"]
            total_invalid += stats["invalid"]
        
        if total_all > 0:
            logger.info("\n‚úÖ T·ªîNG K·∫æT:")
            logger.info("   ‚Ä¢ Total: %s rows", total_all)
            logger.info("   ‚Ä¢ Valid: %s rows (%.1f%%)", 
                       total_valid, 
                       total_valid / total_all * 100)
            logger.info("   ‚Ä¢ Invalid: %s rows (%.1f%%)", 
                       total_invalid, 
                       total_invalid / total_all * 100)
        
        logger.info("=" * 80)


def main():
    print()
    print("‚ïî" + "=" * 78 + "‚ïó")
    print("‚ïë" + " " * 25 + "STEP 3: QUALITY ENGINE" + " " * 32 + "‚ïë")
    print("‚ïë" + " " * 18 + "staging/raw/ ‚Üí staging/clean/ + error/" + " " * 23 + "‚ïë")
    print("‚ïö" + "=" * 78 + "‚ïù")
    print()
    
    pipeline = QualityEnginePipeline()
    
    try:
        validated_data = pipeline.run()
        return validated_data
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Pipeline b·ªã d·ª´ng b·ªüi user (Ctrl+C)")
        return {}
    except Exception as e:
        logger.error("‚ùå Pipeline th·∫•t b·∫°i: %s", e, exc_info=True)
        raise


if __name__ == "__main__":
    main()
