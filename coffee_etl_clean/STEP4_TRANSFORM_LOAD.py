"""
STEP 4: TRANSFORM & LOAD
=========================
ƒê·ªçc staging/clean/*.csv ‚Üí Transform ‚Üí Load v√†o SQL Server staging tables

Lu·ªìng:
- staging/clean/*.csv ‚Üí Transform ‚Üí SQL Server staging tables

Output: Data trong SQL Server staging tables
- staging.khach_hang_csv / staging.khach_hang_sql
- staging.loai_mon_csv / staging.loai_mon_sql
- staging.mon_csv / staging.mon_sql
- staging.nguyen_lieu_csv / staging.nguyen_lieu_sql
- staging.dat_hang_csv / staging.dat_hang_sql
"""

import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List

from etl.db.sql_client import SQLServerClient
from etl.transformers.data_transformer import DataTransformer
from etl.config import settings
from etl.logger import logger


class DatabaseManager:
    """Qu·∫£n l√Ω vi·ªác t·∫°o database + schema + staging tables."""
    
    @staticmethod
    def create_database(db_name: str, sql_client):
        """T·∫°o database m·ªõi n·∫øu ch∆∞a t·ªìn t·∫°i."""
        try:
            check_query = f"""
            SELECT database_id 
            FROM sys.databases 
            WHERE name = '{db_name}'
            """
            result = sql_client.execute_query(check_query)
            
            if result:
                logger.info("   Database '%s' ƒë√£ t·ªìn t·∫°i", db_name)
                return True
            
            create_query = f"CREATE DATABASE [{db_name}]"
            
            sql_client.connection.autocommit = True
            sql_client.cursor.execute(create_query)
            sql_client.connection.autocommit = False
            
            logger.info("   ‚úÖ ƒê√£ t·∫°o database: %s", db_name)
            return True
            
        except Exception as e:
            logger.error("   ‚ùå L·ªói t·∫°o database: %s", e)
            return False
    
    @staticmethod
    def create_staging_schema(sql_client):
        """T·∫°o schema staging."""
        try:
            schema_query = """
            IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'staging')
            BEGIN
                EXEC('CREATE SCHEMA staging')
            END
            """
            sql_client.execute_non_query(schema_query)
            logger.info("   ‚úÖ ƒê√£ t·∫°o schema: staging")
            return True
            
        except Exception as e:
            logger.error("   ‚ùå L·ªói t·∫°o schema: %s", e)
            return False
    
    @staticmethod
    def create_staging_tables(sql_client):
        """T·∫°o c√°c staging tables cho c·∫£ CSV v√† SQL."""
        # T·∫°o t·ª´ng table ri√™ng l·∫ª (kh√¥ng d√πng GO statements)
        tables = [
            # Kh√°ch h√†ng CSV
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'khach_hang_csv' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.khach_hang_csv (
                id INT IDENTITY(1,1) PRIMARY KEY,
                customer_id NVARCHAR(50),
                ho_ten NVARCHAR(200),
                sdt NVARCHAR(20),
                thanh_pho NVARCHAR(100),
                email NVARCHAR(200),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # Kh√°ch h√†ng SQL
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'khach_hang_sql' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.khach_hang_sql (
                id INT IDENTITY(1,1) PRIMARY KEY,
                customer_id NVARCHAR(50),
                ho_ten NVARCHAR(200),
                sdt NVARCHAR(20),
                thanh_pho NVARCHAR(100),
                email NVARCHAR(200),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # Lo·∫°i m√≥n CSV
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'loai_mon_csv' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.loai_mon_csv (
                id INT IDENTITY(1,1) PRIMARY KEY,
                ma_loai NVARCHAR(50),
                ten_loai NVARCHAR(200),
                mo_ta NVARCHAR(500),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # Lo·∫°i m√≥n SQL
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'loai_mon_sql' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.loai_mon_sql (
                id INT IDENTITY(1,1) PRIMARY KEY,
                ma_loai NVARCHAR(50),
                ten_loai NVARCHAR(200),
                mo_ta NVARCHAR(500),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # M√≥n CSV
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'mon_csv' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.mon_csv (
                id INT IDENTITY(1,1) PRIMARY KEY,
                ten_mon NVARCHAR(200),
                loai_id INT,
                gia DECIMAL(18,2),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # M√≥n SQL
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'mon_sql' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.mon_sql (
                id INT IDENTITY(1,1) PRIMARY KEY,
                ten_mon NVARCHAR(200),
                loai_id INT,
                gia DECIMAL(18,2),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # Nguy√™n li·ªáu CSV
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'nguyen_lieu_csv' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.nguyen_lieu_csv (
                id INT IDENTITY(1,1) PRIMARY KEY,
                ma_nguyen_lieu NVARCHAR(50),
                ten_nguyen_lieu NVARCHAR(200),
                so_luong DECIMAL(18,2),
                don_vi NVARCHAR(50),
                gia DECIMAL(18,2),
                nha_cung_cap NVARCHAR(200),
                ngay_nhap DATE,
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # Nguy√™n li·ªáu SQL
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'nguyen_lieu_sql' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.nguyen_lieu_sql (
                id INT IDENTITY(1,1) PRIMARY KEY,
                ma_nguyen_lieu NVARCHAR(50),
                ten_nguyen_lieu NVARCHAR(200),
                so_luong DECIMAL(18,2),
                don_vi NVARCHAR(50),
                gia DECIMAL(18,2),
                nha_cung_cap NVARCHAR(200),
                ngay_nhap DATE,
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # ƒê·∫∑t h√†ng CSV
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dat_hang_csv' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.dat_hang_csv (
                id INT IDENTITY(1,1) PRIMARY KEY,
                khach_hang_id NVARCHAR(50),
                mon_id NVARCHAR(50),
                so_luong INT,
                ngay_dat DATE,
                trang_thai NVARCHAR(50),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """,
            # ƒê·∫∑t h√†ng SQL
            """
            IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dat_hang_sql' AND schema_id = SCHEMA_ID('staging'))
            CREATE TABLE staging.dat_hang_sql (
                id INT IDENTITY(1,1) PRIMARY KEY,
                khach_hang_id NVARCHAR(50),
                mon_id NVARCHAR(50),
                so_luong INT,
                ngay_dat DATE,
                trang_thai NVARCHAR(50),
                extract_time DATETIME,
                loaded_at DATETIME DEFAULT GETDATE()
            )
            """
        ]
        
        try:
            for table_sql in tables:
                sql_client.execute_non_query(table_sql)
            logger.info("   ‚úÖ ƒê√£ t·∫°o staging tables (CSV + SQL)")
            return True
            
        except Exception as e:
            logger.error("   ‚ùå L·ªói t·∫°o tables: %s", e)
            return False


class TransformLoadPipeline:
    """Pipeline Transform & Load - Transform v√† load v√†o SQL Server."""
    
    def __init__(self, db_name: str = None, validated_data: Dict = None):
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.db_name = db_name or f"DB_{self.run_id}"
        
        self.clean_dir = Path("staging") / "clean"
        self.target_db = None
        
        self.stats = {}
        
        # Nh·∫≠n validated data t·ª´ STEP 3 (n·∫øu c√≥)
        self.validated_data = validated_data or {}
    
    def run(self, valid_data_from_memory: Dict[str, List[Dict]] = None):
        """
        Ch·∫°y pipeline Transform & Load.
        
        Args:
            valid_data_from_memory: Dict ch·ª©a valid data t·ª´ STEP 3 (pipeline mode)
                Format: {
                    "khach_hang_csv": [row1, row2, ...],
                    "khach_hang_sql": [row1, row2, ...],
                    ...
                }
                N·∫øu None, s·∫Ω ƒë·ªçc t·ª´ staging/clean/*.csv (standalone mode)
        """
        logger.info("=" * 80)
        logger.info("STEP 4: TRANSFORM & LOAD PIPELINE")
        logger.info("Run ID: %s", self.run_id)
        logger.info("Database: %s", self.db_name)
        
        if valid_data_from_memory:
            logger.info("Mode: Pipeline (data t·ª´ memory)")
        else:
            logger.info("Mode: Standalone (ƒë·ªçc t·ª´ staging/clean/)")
        
        logger.info("=" * 80)
        
        try:
            # Setup database
            logger.info("\nüîπ Setup Database")
            logger.info("-" * 80)
            self.setup_database()
            
            logger.info("\nüîπ Transform & Load")
            logger.info("-" * 80)
            
            if valid_data_from_memory:
                # Pipeline mode: X·ª≠ l√Ω data t·ª´ memory
                self.process_from_memory(valid_data_from_memory)
            else:
                # Standalone mode: ƒê·ªçc t·ª´ files
                self.process_from_files()
            
            self.print_summary()
            
        except Exception as e:
            logger.error("‚ùå L·ªói Transform & Load pipeline: %s", e, exc_info=True)
            raise
        finally:
            if self.target_db:
                self.target_db.close()
    
    def process_from_memory(self, valid_data: Dict[str, List[Dict]]):
        """X·ª≠ l√Ω data tr·ª±c ti·∫øp t·ª´ memory (pipeline mode)."""
        logger.info("üì¶ Processing %s entities t·ª´ memory", len(valid_data))
        
        for entity_source, rows in sorted(valid_data.items()):
            if not rows:
                continue
            
            # Parse entity_source: "khach_hang_csv" ‚Üí entity="khach_hang", source="csv"
            parts = entity_source.rsplit("_", 1)
            if len(parts) == 2:
                entity_type, source = parts
            else:
                entity_type = entity_source
                source = "unknown"
            
            logger.info("\nüì• Processing: %s (source: %s)", entity_type, source)
            logger.info("   Total rows: %s (t·∫•t c·∫£ ƒë√£ pass validation)", len(rows))
            
            # Transform
            self.transform_and_load_rows(entity_type, source, rows)
    
    def process_from_files(self):
        """X·ª≠ l√Ω data t·ª´ files (standalone mode)."""
        # T√¨m t·∫•t c·∫£ clean files
        clean_files = list(self.clean_dir.glob("*.csv"))
        
        if not clean_files:
            logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file n√†o trong staging/clean/")
            return
        
        logger.info("Found %s clean files", len(clean_files))
        
        for clean_file in sorted(clean_files):
            logger.info("\nüì• Processing: %s", clean_file.name)
            self.process_file(clean_file)
    
    def setup_database(self):
        """Setup database v√† staging tables."""
        # K·∫øt n·ªëi master ƒë·ªÉ t·∫°o database
        master_db = SQLServerClient(
            server=f"{settings.TARGET_DB_HOST},{settings.TARGET_DB_PORT}",
            database="master",
            driver=settings.TARGET_DB_DRIVER,
            trusted_connection=settings.TARGET_DB_TRUSTED_CONNECTION,
        )
        
        try:
            master_db.connect()
            logger.info("üì¶ T·∫°o database: %s", self.db_name)
            
            if DatabaseManager.create_database(self.db_name, master_db):
                logger.info("‚úÖ Database ƒë√£ s·∫µn s√†ng")
            else:
                raise Exception("Kh√¥ng th·ªÉ t·∫°o database")
                
        finally:
            master_db.close()
        
        # K·∫øt n·ªëi database m·ªõi
        self.target_db = SQLServerClient(
            server=f"{settings.TARGET_DB_HOST},{settings.TARGET_DB_PORT}",
            database=self.db_name,
            driver=settings.TARGET_DB_DRIVER,
            trusted_connection=settings.TARGET_DB_TRUSTED_CONNECTION,
        )
        
        self.target_db.connect()
        DatabaseManager.create_staging_schema(self.target_db)
        DatabaseManager.create_staging_tables(self.target_db)
        logger.info("‚úÖ Setup database ho√†n th√†nh")
    
    def process_file(self, clean_file: Path):
        """X·ª≠ l√Ω m·ªôt clean file."""
        # Parse file name: entity_source_runid.csv
        file_name = clean_file.stem
        parts = file_name.split("_")
        
        # X√°c ƒë·ªãnh entity type v√† source
        if len(parts) >= 2:
            if parts[-2].isdigit():  # C√≥ run_id
                source = parts[-3]
                entity_type = "_".join(parts[:-3])
            else:
                source = parts[-1]
                entity_type = "_".join(parts[:-1])
        else:
            logger.warning("   ‚ö†Ô∏è  Kh√¥ng parse ƒë∆∞·ª£c file name: %s", file_name)
            return
        
        logger.info("   Entity: %s | Source: %s", entity_type, source)
        
        # ƒê·ªçc clean file
        # CH√ö √ù: Ch·ªâ ƒë·ªçc t·ª´ staging/clean/ - c√°c rows ƒë√£ pass validation
        # C√°c rows c√≥ l·ªói (bao g·ªìm c·ªôt r·ªóng) ƒë√£ b·ªã lo·∫°i b·ªè ·ªü STEP 3
        try:
            with open(clean_file, "r", encoding="utf-8-sig") as f:
                reader = csv.DictReader(f)
                rows = list(reader)
        except Exception as e:
            logger.error("   ‚úó L·ªói ƒë·ªçc file: %s", e)
            return
        
        logger.info("   Total rows: %s (t·∫•t c·∫£ ƒë√£ pass validation)", len(rows))
        
        if not rows:
            logger.info("   ‚ö†Ô∏è  File r·ªóng")
            return
        
        # Transform
        # CH√ö √ù: Ch·ªâ transform c√°c rows VALID t·ª´ clean zone
        logger.info("   üîÑ Transforming...")
        transformed_rows = []
        for row in rows:
            try:
                transformed = DataTransformer.transform(entity_type, row)
                transformed_rows.append(transformed)
            except Exception as e:
                logger.error("   ‚úó L·ªói transform row: %s", e)
        
        logger.info("   ‚úì Transformed: %s rows", len(transformed_rows))
        
        # Load v√†o staging table
        # CH√ö √ù: Ch·ªâ load c√°c rows ƒë√£ pass validation v√† transform th√†nh c√¥ng
        if transformed_rows:
            staging_table = f"staging.{entity_type}_{source}"
            
            try:
                loaded = self.target_db.bulk_insert(
                    table_name=staging_table,
                    data=transformed_rows,
                    batch_size=1000
                )
                logger.info("   ‚úÖ Loaded: %s rows ‚Üí %s", loaded, staging_table)
                
                self.stats[file_name] = {
                    "entity": entity_type,
                    "source": source,
                    "total": len(rows),
                    "loaded": loaded
                }
                
            except Exception as e:
                logger.error("   ‚ùå L·ªói load: %s", e)
    
    def transform_and_load_rows(self, entity_type: str, source: str, rows: List[Dict]):
        """Transform v√† load rows v√†o SQL Server."""
        # Transform
        # CH√ö √ù: Ch·ªâ transform c√°c rows VALID t·ª´ memory
        logger.info("   üîÑ Transforming...")
        transformed_rows = []
        for row in rows:
            try:
                transformed = DataTransformer.transform(entity_type, row)
                transformed_rows.append(transformed)
            except Exception as e:
                logger.error("   ‚úó L·ªói transform row: %s", e)
        
        logger.info("   ‚úì Transformed: %s rows", len(transformed_rows))
        
        # Load v√†o staging table
        # CH√ö √ù: Ch·ªâ load c√°c rows ƒë√£ pass validation v√† transform th√†nh c√¥ng
        if transformed_rows:
            staging_table = f"staging.{entity_type}_{source}"
            
            try:
                loaded = self.target_db.bulk_insert(
                    table_name=staging_table,
                    data=transformed_rows,
                    batch_size=1000
                )
                logger.info("   ‚úÖ Loaded: %s rows ‚Üí %s", loaded, staging_table)
                
                self.stats[f"{entity_type}_{source}"] = {
                    "entity": entity_type,
                    "source": source,
                    "total": len(rows),
                    "loaded": loaded
                }
                
            except Exception as e:
                logger.error("   ‚ùå L·ªói load: %s", e)
    
    def print_summary(self):
        logger.info("\n" + "=" * 80)
        logger.info("üìä TRANSFORM & LOAD SUMMARY")
        logger.info("=" * 80)
        
        logger.info("\nüíæ Database: %s", self.db_name)
        
        logger.info("\nüìä Loaded Data:")
        total_loaded = 0
        
        for file_name, stats in sorted(self.stats.items()):
            logger.info("   ‚Ä¢ staging.%s_%s: %s rows", 
                       stats["entity"], 
                       stats["source"], 
                       stats["loaded"])
            total_loaded += stats["loaded"]
        
        logger.info("\n‚úÖ T·ªîNG: %s rows ƒë√£ load v√†o SQL Server", total_loaded)
        logger.info("=" * 80)


def main():
    print()
    print("‚ïî" + "=" * 78 + "‚ïó")
    print("‚ïë" + " " * 24 + "STEP 4: TRANSFORM & LOAD" + " " * 31 + "‚ïë")
    print("‚ïë" + " " * 18 + "staging/clean/ ‚Üí SQL Server Staging" + " " * 26 + "‚ïë")
    print("‚ïö" + "=" * 78 + "‚ïù")
    print()
    
    # C√≥ th·ªÉ truy·ªÅn db_name t·ª´ command line ho·∫∑c d√πng m·∫∑c ƒë·ªãnh
    import sys
    db_name = sys.argv[1] if len(sys.argv) > 1 else None
    
    pipeline = TransformLoadPipeline(db_name=db_name)
    
    try:
        pipeline.run()
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Pipeline b·ªã d·ª´ng b·ªüi user (Ctrl+C)")
    except Exception as e:
        logger.error("‚ùå Pipeline th·∫•t b·∫°i: %s", e, exc_info=True)
        raise


if __name__ == "__main__":
    main()
